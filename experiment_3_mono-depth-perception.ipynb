{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pix2pix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = \"datasets/kitti/depth_selection/val_selection_cropped/\"\n",
    "\n",
    "if not os.path.isdir(path):\n",
    "    ! mkdir -p datasets/kitti\n",
    "    ! wget -O datasets/kitti.zip https://s3.eu-central-1.amazonaws.com/avg-kitti/data_depth_selection.zip\n",
    "    ! unzip -q -o datasets/kitti.zip -d datasets/kitti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### common dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import random\n",
    "import math\n",
    "from PIL import Image\n",
    "from tqdm import tqdm_notebook\n",
    "from keras_tqdm import TQDMNotebookCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = []\n",
    "ys = []\n",
    "\n",
    "num_samples = 2000\n",
    "\n",
    "for image_path in tqdm_notebook(glob.glob(path + \"image/\" + \"*.png\"), desc=\"Loading Images\"):\n",
    "    x = Image.open(image_path)\n",
    "    x = x.convert('RGB')\n",
    "    \n",
    "    width = math.floor(x.size[0] * 256 / x.size[1])\n",
    "    \n",
    "    x = x.resize((width, 256))\n",
    "\n",
    "    y = Image.open(image_path.replace('/image/', '/groundtruth_depth/').replace('sync_image', 'sync_groundtruth_depth'))\n",
    "    y = y.convert('L')\n",
    "    y = y.resize((width, 256))\n",
    "    \n",
    "    for i in range(random.randint(1, 2)):\n",
    "        x_offset = random.randint(0, width - 256)\n",
    "        x_crop = x.crop((x_offset, 0, x_offset + 256, 256))\n",
    "        y_crop = y.crop((x_offset, 0, x_offset + 256, 256))\n",
    "        \n",
    "        if random.randint(0, 1) > 0:\n",
    "            x_crop = x_crop.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            y_crop = y_crop.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        \n",
    "        xs.append(np.array(x_crop.getdata()).reshape((256, 256, 3)) / 255)\n",
    "        ys.append(np.array(y_crop.getdata()).reshape((256, 256, 1)) / 255)\n",
    "    \n",
    "    if len(xs) >= num_samples:\n",
    "        break;\n",
    "\n",
    "xs = np.array(xs)\n",
    "ys = np.array(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of random images and their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig, ax = plt.subplots(6,6,figsize=(16,16))\n",
    "fig.tight_layout()\n",
    "ax = ax.flatten()\n",
    "\n",
    "for i in range(18):\n",
    "    rand = np.random.randint(len(xs)-1)\n",
    "    x = xs[rand]\n",
    "    y = ys[rand].reshape((256, 256))\n",
    "    \n",
    "    ax[2 * i].imshow(x)\n",
    "    ax[2 * i].set_title(f\"{i}_x\")\n",
    "    ax[2 * i].axis(\"off\")\n",
    "    ax[2 * i + 1].imshow(y)\n",
    "    ax[2 * i + 1].set_title(f\"{i}_y\")\n",
    "    ax[2 * i + 1].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {\"logs/mono-depth-perception/\"} --host 0.0.0.0 --port 8007"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import pix2pix and generate model\n",
    "After creating our dataset we load and create our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run pix2pix.ipynb\n",
    "\n",
    "model = Pix2pix(output_dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allow a consistent split between training and test dataset we split our dataset before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((train_x, train_y), (test_x, test_y)) = model.split_dataset(xs, ys, validation_split=0.025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checkpoint stuff\n",
    "To pause the training process and resume it later we use automatic checkpoints.\n",
    "Before setting up these automatic checkpoints we check if a earlier checkpoint exists and if so load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = 'checkpoints/mono-depth-perception/'\n",
    "checkpoint_path = checkpoint_dir + 'checkpoint-{epoch:04d}.ckpt'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# if a checkpoint exists => build model and load weights\n",
    "if tf.train.latest_checkpoint(checkpoint_dir) != None:\n",
    "    model.build(train_x.shape)\n",
    "    status = model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit model\n",
    "Now, after loading the model and potentially restoring it from a earlier checkpoint, we create our needed callbacks and train the model.\n",
    "\n",
    "To show TQDM progress bars in Jupyter Lab run install the jupyterlab-manager widget before training:\n",
    "``` bash\n",
    "$ jupyter labextension install @jupyter-widgets/jupyterlab-manager\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback for progressbars\n",
    "tqdm_callback = TQDMNotebookCallback(inner_description_update=\"Epoch: {epoch}\")\n",
    "\n",
    "# callback for automatic checkpoints\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=False)\n",
    "\n",
    "# callback for tensorboard\n",
    "tensorboard_calback = tf.keras.callbacks.TensorBoard(log_dir='logs/mono-depth-perception/', histogram_freq=0, write_graph=True, write_images=False, update_freq='epoch')\n",
    "\n",
    "# train the model\n",
    "model.fit(train_x, train_y, batch_size=3, epochs=100, initial_epoch=0, validation_data=(test_x, test_y), callbacks=[tqdm_callback, checkpoint_callback, tensorboard_calback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize results of test data\n",
    "After training the model, we visualize some examples from our tesing dataset including the input, output and expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.predict(test_x, batch_size=10)\n",
    "for i in range(len(out)):\n",
    "    fig, ax = plt.subplots(1,3,figsize=(10,10))\n",
    "    fig.tight_layout()\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    x = test_x[i]\n",
    "    y = test_y[i].reshape((256, 256))\n",
    "    o = out[i].reshape((256, 256))\n",
    "    \n",
    "    ax[0].imshow(x)\n",
    "    ax[0].set_title(\"x\")\n",
    "    ax[0].axis(\"off\")\n",
    "    ax[1].imshow(y)\n",
    "    ax[1].set_title(\"y\")\n",
    "    ax[1].axis(\"off\")\n",
    "    ax[2].imshow(o)\n",
    "    ax[2].set_title(\"g(x)\")\n",
    "    ax[2].axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
