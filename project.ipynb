{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pix2pix in Tensorflow\n",
    "The original repo can be found [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/datasets.md).\n",
    "Dataset That will be used: [facades](http://cmp.felk.cvut.cz/~tylecr1/facade/)\n",
    "\n",
    "## preparation\n",
    "Please download and unzip the [dataset](http://cmp.felk.cvut.cz/~tylecr1/facade/CMP_facade_DB_base.zip) into the directory `./dataset/` before running!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "labels = []\n",
    "\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "path = \"dataset/base/\"\n",
    "\n",
    "for image_path in glob.glob(path + \"*.png\"):\n",
    "    img = Image.open(image_path)\n",
    "    img = img.convert('RGB')\n",
    "    img = img.resize((256, 256))\n",
    "    images.append(np.array(img.getdata()).reshape((256, 256, 3)) / 255)\n",
    "\n",
    "    lbl = Image.open(image_path.replace(\".png\", \".jpg\"))\n",
    "    lbl = lbl.convert('RGB')\n",
    "    lbl = lbl.resize((256, 256))\n",
    "    labels.append(np.array(lbl.getdata()).reshape((256, 256, 3)) / 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of random images and their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig, ax = plt.subplots(6,6,figsize=(24,24))\n",
    "fig.tight_layout()\n",
    "ax = ax.flatten()\n",
    "for i in range(18):\n",
    "    rand = np.random.randint(len(images)-1)\n",
    "    img = images[rand]\n",
    "    lbl = labels[rand]\n",
    "    \n",
    "    ax[2 * i].imshow(img)\n",
    "    ax[2 * i].axis(\"off\")\n",
    "    ax[2 * i + 1].imshow(lbl)\n",
    "    ax[2 * i + 1].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "num_test = 10\n",
    "\n",
    "test_images = np.array(images[:num_test], dtype=np.float32)\n",
    "test_labels = np.array(labels[:num_test], dtype=np.float32)\n",
    "\n",
    "train_images = np.array(images[num_test:], dtype=np.float32)\n",
    "train_labels = np.array(labels[num_test:], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate tf datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "\n",
    "train_dataset = train_dataset.shuffle(buffer_size=50000)\n",
    "batch_size = 10\n",
    "train_dataset = train_dataset.batch(batch_size=batch_size)\n",
    "test_dataset = test_dataset.batch(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import Model\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common layers\n",
    "The paper defines the following types of Layers:\n",
    "- C(k)\n",
    "    - Convolution (k filters; 4x4; stride 2)\n",
    "    - BatchNorm (in testing and training)\n",
    "    - ReLU\n",
    "- CD(k)\n",
    "    - Convolution (k filters; 4x4; stride 2)\n",
    "    - BatchNorm (in testing and training)\n",
    "    - Dropout (50% rate)\n",
    "    - ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C(Layer):\n",
    "    \"\"\"This layer represents the C(k) layer described in the pix2pix paper. The activation function \n",
    "        is a parameter to allow the use of different activation functions like ReLU and leaky ReLU for \n",
    "        encoder and decoder. The sampling_factor gives a factor by which the convolution output will be \n",
    "        sampled up or down. A value of 2 will sample the tensor up by 2. A value of 0.5 will sample the \n",
    "        tensor down by 2.\"\"\"\n",
    "        \n",
    "    def __init__(self, k, activation=None, sampling='down', batchnorm=True):\n",
    "        super(C, self).__init__()\n",
    "        if sampling == 'up':\n",
    "            self.conv = tf.keras.layers.Conv2DTranspose(k, kernel_size=4, strides=2, activation=None, padding='same')\n",
    "        elif sampling == 'down':\n",
    "            self.conv = tf.keras.layers.Conv2D(k, kernel_size=4, strides=2, activation=None, padding='same')\n",
    "        else:\n",
    "            raise AttributeError('illegal sampling mode: \"' + str(sampling) + '\"')\n",
    "            \n",
    "        self.batchnorm = None\n",
    "        if batchnorm:\n",
    "            self.batchnorm = tf.keras.layers.BatchNormalization()\n",
    "            \n",
    "        self.activation = activation\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        if self.batchnorm != None:\n",
    "            x = self.batchnorm(x)\n",
    "        \n",
    "        if self.activation != None:\n",
    "            x = self.activation(x)\n",
    "            \n",
    "        return x\n",
    "\n",
    "class CD(C):\n",
    "    \"\"\"This layer represents the CD(k) layer described in the pix2pix paper. The activation function \n",
    "        is a parameter to allow the use of different activation functions like ReLU and leaky ReLU for \n",
    "        encoder and decoder. The sampling_factor gives a factor by which the convolution output will be \n",
    "        sampled up or down. A value of 2 will sample the tensor up by 2. A value of 0.5 will sample the \n",
    "        tensor down by 2.\"\"\"\n",
    "    \n",
    "    def __init__(self, k, activation=None, sampling=None, batchnorm=True):\n",
    "        super(CD, self).__init__(k, activation, sampling, batchnorm)\n",
    "        self.dropout = tf.keras.layers.Dropout(rate=0.5)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        if self.batchnorm != None:\n",
    "            x = self.batchnorm(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        if self.activation != None:\n",
    "            x = self.activation(x)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the discriminator\n",
    "**16 x 16:**\n",
    "- C64\n",
    "- C128 \n",
    "- conv to 1d\n",
    "- sigmoid\n",
    "\n",
    "**70 x 70:**\n",
    "- C64 \n",
    "- C128 \n",
    "- C256 \n",
    "- C512 \n",
    "- conv to 1d \n",
    "- sigmoid\n",
    "    \n",
    "**286 x 286:**\n",
    "- C64 \n",
    "- C128 \n",
    "- C256 \n",
    "- C512 \n",
    "- C512\n",
    "- C512 \n",
    "- conv to 1d \n",
    "- sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16 x 16 discriminator:\n",
    "class Discriminator16(Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator16, self).__init__()\n",
    "        self.conv1 = C(k=64, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\", batchnorm=False)\n",
    "        self.conv2 = C(k=128, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        \n",
    "        # flatten and dense with one neuron and sigmoid is the same as conv to 1D and sigmoid\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.out = tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid)\n",
    "        \n",
    "    def call(self, x, y):\n",
    "        \"\"\"Calls the discriminator with input x and generator output y\"\"\"\n",
    "        x = tf.keras.layers.concatenate([x, y])\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70 x 70 discriminator:\n",
    "class Discriminator70(Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator70, self).__init__()\n",
    "        self.conv1 = C(k=64, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\", batchnorm=False)\n",
    "        self.conv2 = C(k=128, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        self.conv3 = C(k=256, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        self.conv4 = C(k=512, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        \n",
    "        # flatten and dense with one neuron and sigmoid is the same as conv to 1D and sigmoid\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.out = tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid)\n",
    "        \n",
    "    def call(self, x, y):\n",
    "        \"\"\"Calls the discriminator with input x and generator output y\"\"\"\n",
    "        x = tf.keras.layers.concatenate([x, y])\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 286 x 286 discriminator:\n",
    "class Discriminator286(Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator286, self).__init__()\n",
    "        self.conv1 = C(k=64, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\", batchnorm=False)\n",
    "        self.conv2 = C(k=128, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        self.conv3 = C(k=256, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        self.conv4 = C(k=512, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        self.conv5 = C(k=512, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        self.conv6 = C(k=512, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        \n",
    "        # flatten and dense with one neuron and sigmoid is the same as conv to 1D and sigmoid\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.out = tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid)\n",
    "        \n",
    "    def call(self, x, y):\n",
    "        \"\"\"Calls the discriminator with input x and generator output y\"\"\"\n",
    "        x = tf.keras.layers.concatenate([x, y])\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Generator model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoder\n",
    "\n",
    "encoder: (Leaky ReLU (slope = 0.2))\n",
    "- C64\n",
    "- C128\n",
    "- C256\n",
    "- C512\n",
    "- C512\n",
    "- C512\n",
    "- C512\n",
    "- C512\n",
    "\n",
    "UNet decoder: (ReLU)\n",
    "- CD512\n",
    "- CD1024\n",
    "- CD1024\n",
    "- C1024\n",
    "- C1024\n",
    "- C512\n",
    "- C256\n",
    "- C128\n",
    "- reduction to output channels\n",
    "- tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(Model):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # encoder:\n",
    "        self.enc_conv1 = C(k=64, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\", batchnorm=False)\n",
    "        self.enc_conv2 = C(k=128, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        self.enc_conv3 = C(k=256, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        self.enc_conv4 = C(k=512, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        self.enc_conv5 = C(k=512, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        self.enc_conv6 = C(k=512, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        self.enc_conv7 = C(k=512, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        self.enc_conv8 = C(k=512, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        \n",
    "        # decoder\n",
    "        self.dec_conv1 = CD(k=512, activation=tf.keras.activations.relu, sampling=\"up\")\n",
    "        self.dec_conv2 = CD(k=1024, activation=tf.keras.activations.relu, sampling=\"up\")\n",
    "        self.dec_conv3 = CD(k=1024, activation=tf.keras.activations.relu, sampling=\"up\")\n",
    "        self.dec_conv4 = C(k=1024, activation=tf.keras.activations.relu, sampling=\"up\")\n",
    "        self.dec_conv5 = C(k=1024, activation=tf.keras.activations.relu, sampling=\"up\")\n",
    "        self.dec_conv6 = C(k=512, activation=tf.keras.activations.relu, sampling=\"up\")\n",
    "        self.dec_conv7 = C(k=256, activation=tf.keras.activations.relu, sampling=\"up\")\n",
    "        self.dec_conv8 = C(k=128, activation=tf.keras.activations.relu, sampling=\"up\")\n",
    "        \n",
    "        self.out = tf.keras.layers.Conv2D(3, kernel_size=3, strides=1, activation=tf.keras.activations.tanh, padding='same')\n",
    "        \n",
    "    def call(self, x):\n",
    "        # encoder\n",
    "        x1 = self.enc_conv1(x)\n",
    "        x2 = self.enc_conv2(x1)\n",
    "        x3 = self.enc_conv3(x2)\n",
    "        x4 = self.enc_conv4(x3)\n",
    "        x5 = self.enc_conv5(x4)\n",
    "        x6 = self.enc_conv6(x5)\n",
    "        x7 = self.enc_conv7(x6)\n",
    "        x8 = self.enc_conv8(x7)\n",
    "        \n",
    "        #decoder\n",
    "        x = self.dec_conv1(x8)\n",
    "        x = self.dec_conv2(tf.keras.layers.concatenate([x, x7]))\n",
    "        x = self.dec_conv3(tf.keras.layers.concatenate([x, x6]))\n",
    "        x = self.dec_conv4(tf.keras.layers.concatenate([x, x5]))\n",
    "        x = self.dec_conv5(tf.keras.layers.concatenate([x, x4]))\n",
    "        x = self.dec_conv6(tf.keras.layers.concatenate([x, x3]))\n",
    "        x = self.dec_conv7(tf.keras.layers.concatenate([x, x2]))\n",
    "        x = self.dec_conv8(tf.keras.layers.concatenate([x, x1]))\n",
    "        \n",
    "        # get three channels\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Discriminator70()\n",
    "g = Generator()\n",
    "\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(200):\n",
    "    print(\"starting epoch\", epoch)\n",
    "    \n",
    "    for (x, t) in train_dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            real = d(x, t)\n",
    "            fake = d(x, g(x))\n",
    "            \n",
    "            loss = cross_entropy(tf.ones_like(real), real) + cross_entropy(tf.zeros_like(fake), fake)\n",
    "        \n",
    "        gradients = tape.gradient(loss, d.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, d.trainable_variables))\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            y = g(x)\n",
    "            real = d(x, t)\n",
    "            fake = d(x, y)\n",
    "            \n",
    "            gan_loss = cross_entropy(tf.ones_like(fake), fake)\n",
    "            l1_loss = tf.reduce_mean(tf.abs(t - y))\n",
    "            loss = gan_loss + l1_loss\n",
    "            \n",
    "        gradients = tape.gradient(loss, g.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, g.trainable_variables))\n",
    "    \n",
    "    for (x, t) in test_dataset:\n",
    "        y = g(x)\n",
    "        if epoch % 5 == 0:\n",
    "            plt.imshow(x.numpy()[0])\n",
    "            plt.show()\n",
    "            plt.imshow(t.numpy()[0])\n",
    "            plt.show()\n",
    "            plt.imshow(y.numpy()[0])\n",
    "            plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (x, t) in test_dataset:\n",
    "    y = g(x)\n",
    "    for i in range(x.numpy().shape[0]):\n",
    "        plt.imshow(x.numpy()[i])\n",
    "        plt.show()\n",
    "        plt.imshow(t.numpy()[i])\n",
    "        plt.show()\n",
    "        plt.imshow(y.numpy()[i])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
