{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pix2Pix\n",
    "## by Malte Kleine-Piening, Raphael Marx and Christopher Br√∂cker (20.04.2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Motivation / Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our group decided to implement the pix2pix paper from Isola et al. (2018). \n",
    "\n",
    "At first we were a little confused because at first glance the pix2pix model seemed to be able to do everything. It could e.g. translate from edges to handbags, from aerial images to maps, from labels to facades and from black-and-white to color. \n",
    "\n",
    "Other papers that we looked at like \"Colorful Image Colorization\" only focused on one of these tasks and built a spezialized architecture just for that task. \n",
    "\n",
    "The authors of the pix2pix paper, however,  showed that the task in a lot of image translation problems  is very similar and boils down to a more general approch: predicting pixels from pixels.\n",
    "Therefor they introduce a general framework that can be used on numerous different tasks. \n",
    "\n",
    "We quite like this wide applicability and the ease of adopting the model for a whole new task. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Background knowledge (reference to most important publications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the past if you wanted to do any image-to-image translation you had to build a specialized tool for that specific task.\n",
    "With the introduction of CNNs and eventually GANs image prediction became a lot easier to implement.\n",
    "\n",
    "Because we are going to talk more about Generative Adversarial Networks (GANs) in the upcoming sections we should briefly explain what they are. [REFERENCE TO GAN PAPER]\n",
    "\n",
    "The basic idea is that you have 2 neural networks that are trying to work against each other and therefore improve themselves. One NN is called the Generator and aims to produce the best fake data that it can while the second NN is called the Discriminator which gets both this fake input and real input from the dataset and tries to differentiate between the two. In our case the data that is being produced are images. \n",
    "\n",
    "If the Generator produces an image that fools the Discriminator it knows that it was a good sample and therefore improves. \n",
    "\n",
    "The pix2pix paper doesn't use the base GANs but a variation called cGans which stands for conditional GANs. \n",
    "cGans don't just learn a mapping from random noise to output image but from observed image x AND random noise to output image. \n",
    "They also (is it also or because the stuff above?) learn a structured loss which penalizes the joint configuration of the output. \n",
    "\n",
    "It is also important to mention that the pix2pix paper is not the first paper to use cGANs but the first one to not build something specifically for one task.\n",
    "\n",
    "As we will talk more in section 3 \"Model\" below the architecture of the pix2pix model is based on [REFERENCE to 44]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "* focus on the c in cGAN\n",
    "* conditional GANs learn a structured loss which penalizes the joint configuration of the output\n",
    "* U-NET is notable\n",
    "* PatchGAN classifier (from 38)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Visualization and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### facades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### winter to summer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sparse mono depth perception "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sparse to dense depthmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### image unblurring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}