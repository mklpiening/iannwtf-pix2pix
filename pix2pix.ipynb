{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pix2pix in Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import Model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common layers\n",
    "The paper defines the following types of Layers:\n",
    "- C(k)\n",
    "    - Convolution (k filters; 4x4; stride 2)\n",
    "    - BatchNorm (in testing and training)\n",
    "    - ReLU\n",
    "- CD(k)\n",
    "    - Convolution (k filters; 4x4; stride 2)\n",
    "    - BatchNorm (in testing and training)\n",
    "    - Dropout (50% rate)\n",
    "    - ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C(Layer):\n",
    "    \"\"\"This layer represents the C(k) layer described in the pix2pix paper. The activation function \n",
    "        is a parameter to allow the use of different activation functions like ReLU and leaky ReLU for \n",
    "        encoder and decoder. The sampling_factor gives a factor by which the convolution output will be \n",
    "        sampled up or down. A value of 2 will sample the tensor up by 2. A value of 0.5 will sample the \n",
    "        tensor down by 2.\"\"\"\n",
    "        \n",
    "    def __init__(self, k, activation=None, sampling='down', batchnorm=True):\n",
    "        super(C, self).__init__()\n",
    "        if sampling == 'up':\n",
    "            self.conv = tf.keras.layers.Conv2DTranspose(k, kernel_size=4, strides=2, activation=None, padding='same')\n",
    "        elif sampling == 'down':\n",
    "            self.conv = tf.keras.layers.Conv2D(k, kernel_size=4, strides=2, activation=None, padding='same')\n",
    "        else:\n",
    "            raise AttributeError('illegal sampling mode: \"' + str(sampling) + '\"')\n",
    "            \n",
    "        self.batchnorm = None\n",
    "        if batchnorm:\n",
    "            self.batchnorm = tf.keras.layers.BatchNormalization()\n",
    "            \n",
    "        self.activation = activation\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        if self.batchnorm != None:\n",
    "            x = self.batchnorm(x)\n",
    "        \n",
    "        if self.activation != None:\n",
    "            x = self.activation(x)\n",
    "            \n",
    "        return x\n",
    "\n",
    "class CD(C):\n",
    "    \"\"\"This layer represents the CD(k) layer described in the pix2pix paper. The activation function \n",
    "        is a parameter to allow the use of different activation functions like ReLU and leaky ReLU for \n",
    "        encoder and decoder. The sampling_factor gives a factor by which the convolution output will be \n",
    "        sampled up or down. A value of 2 will sample the tensor up by 2. A value of 0.5 will sample the \n",
    "        tensor down by 2.\"\"\"\n",
    "    \n",
    "    def __init__(self, k, activation=None, sampling=None, batchnorm=True):\n",
    "        super(CD, self).__init__(k, activation, sampling, batchnorm)\n",
    "        self.dropout = tf.keras.layers.Dropout(rate=0.5)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        if self.batchnorm != None:\n",
    "            x = self.batchnorm(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        if self.activation != None:\n",
    "            x = self.activation(x)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the discriminator\n",
    "**16 x 16:**\n",
    "- C64\n",
    "- C128 \n",
    "- conv to 1d\n",
    "- sigmoid\n",
    "\n",
    "**70 x 70:**\n",
    "- C64 \n",
    "- C128 \n",
    "- C256 \n",
    "- C512 \n",
    "- conv to 1d \n",
    "- sigmoid\n",
    "    \n",
    "**286 x 286:**\n",
    "- C64 \n",
    "- C128 \n",
    "- C256 \n",
    "- C512 \n",
    "- C512\n",
    "- C512 \n",
    "- conv to 1d \n",
    "- sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16 x 16 discriminator:\n",
    "class Discriminator16(Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator16, self).__init__()\n",
    "        self.conv1 = C(k=64, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\", batchnorm=False)\n",
    "        self.conv2 = C(k=128, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        \n",
    "        # flatten and dense with one neuron and sigmoid is the same as conv to 1D and sigmoid\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.out = tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid)\n",
    "        \n",
    "    def call(self, x, y):\n",
    "        \"\"\"Calls the discriminator with input x and generator output y\"\"\"\n",
    "        x = tf.keras.layers.concatenate([x, y])\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70 x 70 discriminator:\n",
    "class Discriminator70(Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator70, self).__init__()\n",
    "        self.conv1 = C(k=64, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\", batchnorm=False)\n",
    "        self.conv2 = C(k=128, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        self.conv3 = C(k=256, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        self.conv4 = C(k=512, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        \n",
    "        # flatten and dense with one neuron and sigmoid is the same as conv to 1D and sigmoid\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.out = tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid)\n",
    "        \n",
    "    def call(self, x, y):\n",
    "        \"\"\"Calls the discriminator with input x and generator output y\"\"\"\n",
    "        x = tf.keras.layers.concatenate([x, y])\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 286 x 286 discriminator:\n",
    "class Discriminator286(Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator286, self).__init__()\n",
    "        self.conv1 = C(k=64, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\", batchnorm=False)\n",
    "        self.conv2 = C(k=128, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        self.conv3 = C(k=256, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        self.conv4 = C(k=512, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        self.conv5 = C(k=512, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        self.conv6 = C(k=512, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        \n",
    "        # flatten and dense with one neuron and sigmoid is the same as conv to 1D and sigmoid\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.out = tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid)\n",
    "        \n",
    "    def call(self, x, y):\n",
    "        \"\"\"Calls the discriminator with input x and generator output y\"\"\"\n",
    "        x = tf.keras.layers.concatenate([x, y])\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Generator model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoder\n",
    "\n",
    "encoder: (Leaky ReLU (slope = 0.2))\n",
    "- C64\n",
    "- C128\n",
    "- C256\n",
    "- C512\n",
    "- C512\n",
    "- C512\n",
    "- C512\n",
    "- C512\n",
    "\n",
    "UNet decoder: (ReLU)\n",
    "- CD512\n",
    "- CD1024\n",
    "- CD1024\n",
    "- C1024\n",
    "- C1024\n",
    "- C512\n",
    "- C256\n",
    "- C128\n",
    "- reduction to output channels\n",
    "- tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(Model):\n",
    "    def __init__(self, output_dim=3):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # encoder:\n",
    "        self.enc_conv1 = C(k=64, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\", batchnorm=False)\n",
    "        self.enc_conv2 = C(k=128, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        self.enc_conv3 = C(k=256, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        self.enc_conv4 = C(k=512, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        self.enc_conv5 = C(k=512, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        self.enc_conv6 = C(k=512, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        self.enc_conv7 = C(k=512, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        self.enc_conv8 = C(k=512, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        \n",
    "        # decoder\n",
    "        self.dec_conv1 = CD(k=512, activation=tf.keras.activations.relu, sampling=\"up\")\n",
    "        self.dec_conv2 = CD(k=1024, activation=tf.keras.activations.relu, sampling=\"up\")\n",
    "        self.dec_conv3 = CD(k=1024, activation=tf.keras.activations.relu, sampling=\"up\")\n",
    "        self.dec_conv4 = C(k=1024, activation=tf.keras.activations.relu, sampling=\"up\")\n",
    "        self.dec_conv5 = C(k=1024, activation=tf.keras.activations.relu, sampling=\"up\")\n",
    "        self.dec_conv6 = C(k=512, activation=tf.keras.activations.relu, sampling=\"up\")\n",
    "        self.dec_conv7 = C(k=256, activation=tf.keras.activations.relu, sampling=\"up\")\n",
    "        self.dec_conv8 = C(k=128, activation=tf.keras.activations.relu, sampling=\"up\")\n",
    "        \n",
    "        self.out = tf.keras.layers.Conv2D(output_dim, kernel_size=3, strides=1, activation=tf.keras.activations.tanh, padding='same')\n",
    "        \n",
    "    def call(self, x):\n",
    "        # encoder\n",
    "        x1 = self.enc_conv1(x)\n",
    "        x2 = self.enc_conv2(x1)\n",
    "        x3 = self.enc_conv3(x2)\n",
    "        x4 = self.enc_conv4(x3)\n",
    "        x5 = self.enc_conv5(x4)\n",
    "        x6 = self.enc_conv6(x5)\n",
    "        x7 = self.enc_conv7(x6)\n",
    "        x8 = self.enc_conv8(x7)\n",
    "        \n",
    "        #decoder\n",
    "        x = self.dec_conv1(x8)\n",
    "        x = self.dec_conv2(tf.keras.layers.concatenate([x, x7]))\n",
    "        x = self.dec_conv3(tf.keras.layers.concatenate([x, x6]))\n",
    "        x = self.dec_conv4(tf.keras.layers.concatenate([x, x5]))\n",
    "        x = self.dec_conv5(tf.keras.layers.concatenate([x, x4]))\n",
    "        x = self.dec_conv6(tf.keras.layers.concatenate([x, x3]))\n",
    "        x = self.dec_conv7(tf.keras.layers.concatenate([x, x2]))\n",
    "        x = self.dec_conv8(tf.keras.layers.concatenate([x, x1]))\n",
    "        \n",
    "        # get three channels\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the combined pix2pix model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9565c3355de6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mPix2pix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\" \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDiscriminator70\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPix2pix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Model' is not defined"
     ]
    }
   ],
   "source": [
    "class Pix2pix(Model):\n",
    "    \"\"\" \"\"\"\n",
    "    def __init__(self, discriminator=Discriminator70(), output_dim=3):\n",
    "        super(Pix2pix, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.g = Generator(output_dim=output_dim)\n",
    "        self.d = discriminator\n",
    "        \n",
    "        self.cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.999)\n",
    "    \n",
    "    def _disc_loss(self, disc_real, disc_fake):\n",
    "        \"\"\" \"\"\"\n",
    "        return self.cross_entropy(tf.ones_like(disc_real), disc_real) + self.cross_entropy(tf.zeros_like(disc_fake), disc_fake)\n",
    "    \n",
    "    def _gen_loss(self, y, generated, disc_fake):\n",
    "        \"\"\" \"\"\"\n",
    "        gan_loss = self.cross_entropy(tf.ones_like(disc_fake), disc_fake)\n",
    "        l1_loss = tf.reduce_mean(tf.abs(y - generated))\n",
    "        return gan_loss + (6 / self.output_dim) * l1_loss\n",
    "    \n",
    "    def split_dataset(self, x, y, validation_split=0.0):\n",
    "        \"\"\" \"\"\"\n",
    "        num_test = math.floor(x.shape[0] * validation_split)\n",
    "\n",
    "        test_x = np.array(x[:num_test], dtype=np.float32)\n",
    "        test_y = np.array(y[:num_test], dtype=np.float32)\n",
    "\n",
    "        train_x = np.array(x[num_test:], dtype=np.float32)\n",
    "        train_y = np.array(y[num_test:], dtype=np.float32)\n",
    "        \n",
    "        return ((train_x, train_y), (test_x, test_y))\n",
    "    \n",
    "    def predict_on_batch(self, x):\n",
    "        \"\"\" \"\"\"\n",
    "        return self.g(x)\n",
    "    \n",
    "    def predict(self, x, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False):\n",
    "        \"\"\" \"\"\"\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "        if batch_size == None:\n",
    "            dataset = dataset.batch(batch_size=1)\n",
    "        else:\n",
    "            dataset = dataset.batch(batch_size=batch_size)\n",
    "        \n",
    "        if callbacks != None:\n",
    "            for callback in callbacks:\n",
    "                callback.on_predict_begin()\n",
    "        \n",
    "        result = np.array([])\n",
    "        for n, x1 in dataset.enumerate():\n",
    "            if callbacks != None:\n",
    "                for callback in callbacks:\n",
    "                    callback.on_predict_batch_begin(n)\n",
    "                    \n",
    "            output = self.predict_on_batch(x1)\n",
    "            if result.size == 0:\n",
    "                result = output.numpy()\n",
    "            else:\n",
    "                result = np.concatenate((result, output.numpy()), axis=0)\n",
    "            \n",
    "            if callbacks != None:\n",
    "                for callback in callbacks:\n",
    "                    callback.on_predict_batch_end(n)\n",
    "            \n",
    "            if steps != None and n >= steps:\n",
    "                break\n",
    "        \n",
    "        if callbacks != None:\n",
    "            for callback in callbacks:\n",
    "                callback.on_predict_end()\n",
    "                \n",
    "        return result\n",
    "    \n",
    "    def fit(self, x, y, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False):\n",
    "        \"\"\" \"\"\"\n",
    "        # get or generate validation data\n",
    "        if validation_data != None:\n",
    "            train_x = x\n",
    "            train_y = y\n",
    "            \n",
    "            test_x = validation_data[0]\n",
    "            test_y = validation_data[1]\n",
    "        else:\n",
    "            ((train_x, train_y), (test_x, test_y)) = self.split_dataset(x, y, validation_split=validation_split)\n",
    "        \n",
    "        # generate tf datasets from data\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y))\n",
    "\n",
    "        if shuffle:\n",
    "            train_dataset = train_dataset.shuffle(buffer_size=50000)\n",
    "\n",
    "        if batch_size == None:\n",
    "            train_dataset = train_dataset.batch(batch_size=1)\n",
    "        else:\n",
    "            train_dataset = train_dataset.batch(batch_size=batch_size)\n",
    "        test_dataset = test_dataset.batch(10000)\n",
    "        \n",
    "        # prepare callbacks\n",
    "        if callbacks != None:\n",
    "            params = {\n",
    "                'batch_size': batch_size,\n",
    "                'epochs': epochs,\n",
    "                'steps': steps_per_epoch,\n",
    "                'samples': train_x.shape[0],\n",
    "                'verbose': verbose,\n",
    "                'do_validation': validation_steps != None,\n",
    "                'metrics': []\n",
    "            }\n",
    "            for callback in callbacks:\n",
    "                callback.set_params(params)\n",
    "                \n",
    "                callback.set_model(self)\n",
    "        \n",
    "        # train\n",
    "        if callbacks != None:\n",
    "            for callback in callbacks:\n",
    "                callback.on_train_begin()\n",
    "        \n",
    "        for epoch in range(initial_epoch, epochs):\n",
    "            if callbacks != None:\n",
    "                for callback in callbacks:\n",
    "                    callback.on_epoch_begin(epoch)\n",
    "                    \n",
    "            for n, (x, y) in train_dataset.enumerate():\n",
    "                if callbacks != None:\n",
    "                    for callback in callbacks:\n",
    "                        callback.on_batch_begin(n)\n",
    "                        \n",
    "                model.train_on_batch(x, y)\n",
    "                \n",
    "                if callbacks != None:\n",
    "                    for callback in callbacks:\n",
    "                        callback.on_batch_end(n, logs={'size': x.shape[0]})\n",
    "                \n",
    "                if steps_per_epoch != None and n >= steps_per_epoch:\n",
    "                    break\n",
    "                    \n",
    "            if callbacks != None:\n",
    "                for callback in callbacks:\n",
    "                    callback.on_epoch_end(epoch)\n",
    "        \n",
    "        if callbacks != None:\n",
    "            for callback in callbacks:\n",
    "                callback.on_train_end()\n",
    "    \n",
    "    def train_on_batch(self, x, y, sample_weight=None, class_weight=None, reset_metrics=True):\n",
    "        \"\"\" \"\"\"\n",
    "        # train discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            generated = self.g(x)\n",
    "            disc_real = self.d(x, y)\n",
    "            disc_fake = self.d(x, generated)\n",
    "            \n",
    "            loss = self._disc_loss(disc_real, disc_fake)\n",
    "            \n",
    "        gradients = tape.gradient(loss, self.d.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.d.trainable_variables))\n",
    "        \n",
    "        # train generator\n",
    "        with tf.GradientTape() as tape:\n",
    "            generated = self.g(x)\n",
    "            disc_fake = self.d(x, generated)\n",
    "            \n",
    "            loss = self._gen_loss(y, generated, disc_fake)\n",
    "            \n",
    "        gradients = tape.gradient(loss, self.g.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.g.trainable_variables))\n",
    "        \n",
    "    def test_on_batch(self, x, y, sample_weight=None, reset_metrics=True):\n",
    "        \"\"\" \"\"\"\n",
    "        generated = self.g(x)\n",
    "        disc_real = self.d(x, y)\n",
    "        disc_fake = self.d(x, generated)\n",
    "\n",
    "        disc_loss = self._disc_loss(disc_real, disc_fake)\n",
    "        gen_loss = self._gen_loss(y, generated, disc_fake)\n",
    "        \n",
    "        return np.array([disc_loss, gen_loss])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
