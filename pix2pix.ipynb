{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pix2pix in Tensorflow\n",
    "This notebook implements the network nrchitecture and training process of the Pix2Pix approach of generating new images from gven images.\n",
    "The architecture is based on the conditional GAN (Generative Adversarial Network) which adds the input image as an input of the discriminator which is only an input of the generator in conventional GAN architectures.\n",
    "\n",
    "A bug point of the Pix2Pix approach is its good performance in versatile problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common packages\n",
    "\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import Model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of Pix2Pix\n",
    "According to the GAN architecture, Pix2Pix is split up into two parts which are the generator and the discriminator. \n",
    "\n",
    "As the name suggests, the generator generates images from a given input image. The output of the generator will be the output later on.\n",
    "\n",
    "The discriminator however classifies images as real or fake. It is used aside the l1 loss as of the loss function of the generator.\n",
    "\n",
    "While training the discriminator tries to maximize the loss of the generator while the generator tries to minimize it.\n",
    "To do so, the discriminator minimizes the loss for correctly classifying the real image as real (0) and the generated image as fake (1). The generator simply minimizes the output of the discriminator plus the l1 loss. The l1 loss is included to improve the output image quality. These improvements are shown in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common layers\n",
    "The paper defines the following types of Layers wich we implement before taking a look at the model:\n",
    "- C(k)\n",
    "    - Convolution (k filters; 4x4; stride 2)\n",
    "    - BatchNorm (in testing and training)\n",
    "    - ReLU\n",
    "- CD(k)\n",
    "    - Convolution (k filters; 4x4; stride 2)\n",
    "    - BatchNorm (in testing and training)\n",
    "    - Dropout (50% rate)\n",
    "    - ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C(Layer):\n",
    "    \"\"\"This layer represents the C(k) layer described in the pix2pix paper. The activation function \n",
    "        is a parameter to allow the use of different activation functions like ReLU and leaky ReLU for \n",
    "        encoder and decoder. The sampling_factor gives a factor by which the convolution output will be \n",
    "        sampled up or down. A value of 2 will sample the tensor up by 2. A value of 0.5 will sample the \n",
    "        tensor down by 2.\"\"\"\n",
    "        \n",
    "    def __init__(self, k, activation=None, sampling='down', batchnorm=True):\n",
    "        super(C, self).__init__()\n",
    "        if sampling == 'up':\n",
    "            self.conv = tf.keras.layers.Conv2DTranspose(k, kernel_size=4, strides=2, activation=None, padding='same')\n",
    "        elif sampling == 'down':\n",
    "            self.conv = tf.keras.layers.Conv2D(k, kernel_size=4, strides=2, activation=None, padding='same')\n",
    "        else:\n",
    "            raise AttributeError('illegal sampling mode: \"' + str(sampling) + '\"')\n",
    "            \n",
    "        self.batchnorm = None\n",
    "        if batchnorm:\n",
    "            self.batchnorm = tf.keras.layers.BatchNormalization()\n",
    "            \n",
    "        self.activation = activation\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        if self.batchnorm != None:\n",
    "            x = self.batchnorm(x)\n",
    "        \n",
    "        if self.activation != None:\n",
    "            x = self.activation(x)\n",
    "            \n",
    "        return x\n",
    "\n",
    "class CD(C):\n",
    "    \"\"\"This layer represents the CD(k) layer described in the pix2pix paper. The activation function \n",
    "        is a parameter to allow the use of different activation functions like ReLU and leaky ReLU for \n",
    "        encoder and decoder. The sampling_factor gives a factor by which the convolution output will be \n",
    "        sampled up or down. A value of 2 will sample the tensor up by 2. A value of 0.5 will sample the \n",
    "        tensor down by 2.\"\"\"\n",
    "    \n",
    "    def __init__(self, k, activation=None, sampling=None, batchnorm=True):\n",
    "        super(CD, self).__init__(k, activation, sampling, batchnorm)\n",
    "        self.dropout = tf.keras.layers.Dropout(rate=0.5)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv(x)\n",
    "        \n",
    "        if self.batchnorm != None:\n",
    "            x = self.batchnorm(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        if self.activation != None:\n",
    "            x = self.activation(x)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the discriminator\n",
    "The paper presents different discriminator sizes for different result performances. The paper shows that the 70x70 discriminator shows a great balance between quality and training time.\n",
    "\n",
    "The following lists show the architectures of the different discriminators using the Layers defines earlier. All convolutions used here downsample the image and all but the first convolution layer use batchnorm.\n",
    "\n",
    "**16 x 16:**\n",
    "- C64\n",
    "- C128 \n",
    "- conv to 1d\n",
    "- sigmoid\n",
    "\n",
    "**70 x 70:**\n",
    "- C64 \n",
    "- C128 \n",
    "- C256 \n",
    "- C512 \n",
    "- conv to 1d \n",
    "- sigmoid\n",
    "    \n",
    "**286 x 286:**\n",
    "- C64 \n",
    "- C128 \n",
    "- C256 \n",
    "- C512 \n",
    "- C512\n",
    "- C512 \n",
    "- conv to 1d \n",
    "- sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16 x 16 discriminator:\n",
    "class Discriminator16(Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator16, self).__init__()\n",
    "        self.conv1 = C(k=64, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\", batchnorm=False)\n",
    "        self.conv2 = C(k=128, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        \n",
    "        # flatten and dense with one neuron and sigmoid is the same as conv to 1D and sigmoid\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.out = tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid)\n",
    "        \n",
    "    def call(self, x, y):\n",
    "        \"\"\"Calls the discriminator with input x and generator output y\"\"\"\n",
    "        x = tf.keras.layers.concatenate([x, y])\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70 x 70 discriminator:\n",
    "class Discriminator70(Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator70, self).__init__()\n",
    "        self.conv1 = C(k=64, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\", batchnorm=False)\n",
    "        self.conv2 = C(k=128, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        self.conv3 = C(k=256, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        self.conv4 = C(k=512, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        \n",
    "        # flatten and dense with one neuron and sigmoid is the same as conv to 1D and sigmoid\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.out = tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid)\n",
    "        \n",
    "    def call(self, x, y):\n",
    "        \"\"\"Calls the discriminator with input x and generator output y\"\"\"\n",
    "        x = tf.keras.layers.concatenate([x, y])\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 286 x 286 discriminator:\n",
    "class Discriminator286(Model):\n",
    "    def __init__(self):\n",
    "        super(Discriminator286, self).__init__()\n",
    "        self.conv1 = C(k=64, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\", batchnorm=False)\n",
    "        self.conv2 = C(k=128, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        self.conv3 = C(k=256, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        self.conv4 = C(k=512, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        self.conv5 = C(k=512, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        self.conv6 = C(k=512, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        \n",
    "        # flatten and dense with one neuron and sigmoid is the same as conv to 1D and sigmoid\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.out = tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid)\n",
    "        \n",
    "    def call(self, x, y):\n",
    "        \"\"\"Calls the discriminator with input x and generator output y\"\"\"\n",
    "        x = tf.keras.layers.concatenate([x, y])\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Generator model\n",
    "The generator is based on the widely used autoencoder structure. This means that the generator consists of an encoder and an decoder.\n",
    "Like for the discriminator, the following lists describe the encoder and decoder generator.\n",
    "\n",
    "**encoder: (Leaky ReLU (slope = 0.2))**\n",
    "- C64\n",
    "- C128\n",
    "- C256\n",
    "- C512\n",
    "- C512\n",
    "- C512\n",
    "- C512\n",
    "- C512\n",
    "\n",
    "All convultions of the encoder downsample the image while the decoder convolutions upsample the images. As with the encoder, all convolutions apply batchnorm while the first convolution does not.\n",
    "\n",
    "**UNet decoder: (ReLU)**\n",
    "- CD512\n",
    "- CD1024\n",
    "- CD1024\n",
    "- C1024\n",
    "- C1024\n",
    "- C512\n",
    "- C256\n",
    "- C128\n",
    "- reduction to output channels\n",
    "- tanh\n",
    "\n",
    "All decoder convolutions upsample the image and apply batchnorm.\n",
    "As said before, we use the UNet structure for the generator. This means that we have connections from the i-th layer to the (n - i)-th layer of the autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(Model):\n",
    "    def __init__(self, output_dim=3):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # encoder:\n",
    "        self.enc_conv1 = C(k=64, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\", batchnorm=False)\n",
    "        self.enc_conv2 = C(k=128, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        self.enc_conv3 = C(k=256, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        self.enc_conv4 = C(k=512, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        self.enc_conv5 = C(k=512, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        self.enc_conv6 = C(k=512, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        self.enc_conv7 = C(k=512, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        self.enc_conv8 = C(k=512, activation=tf.keras.layers.LeakyReLU(alpha=0.2), sampling=\"down\")\n",
    "        \n",
    "        # decoder\n",
    "        self.dec_conv1 = CD(k=512, activation=tf.keras.activations.relu, sampling=\"up\")\n",
    "        self.dec_conv2 = CD(k=1024, activation=tf.keras.activations.relu, sampling=\"up\")\n",
    "        self.dec_conv3 = CD(k=1024, activation=tf.keras.activations.relu, sampling=\"up\")\n",
    "        self.dec_conv4 = C(k=1024, activation=tf.keras.activations.relu, sampling=\"up\")\n",
    "        self.dec_conv5 = C(k=1024, activation=tf.keras.activations.relu, sampling=\"up\")\n",
    "        self.dec_conv6 = C(k=512, activation=tf.keras.activations.relu, sampling=\"up\")\n",
    "        self.dec_conv7 = C(k=256, activation=tf.keras.activations.relu, sampling=\"up\")\n",
    "        self.dec_conv8 = C(k=128, activation=tf.keras.activations.relu, sampling=\"up\")\n",
    "        \n",
    "        self.out = tf.keras.layers.Conv2D(output_dim, kernel_size=3, strides=1, activation=tf.keras.activations.tanh, padding='same')\n",
    "        \n",
    "    def call(self, x):\n",
    "        # encoder\n",
    "        x1 = self.enc_conv1(x)\n",
    "        x2 = self.enc_conv2(x1)\n",
    "        x3 = self.enc_conv3(x2)\n",
    "        x4 = self.enc_conv4(x3)\n",
    "        x5 = self.enc_conv5(x4)\n",
    "        x6 = self.enc_conv6(x5)\n",
    "        x7 = self.enc_conv7(x6)\n",
    "        x8 = self.enc_conv8(x7)\n",
    "        \n",
    "        #decoder\n",
    "        x = self.dec_conv1(x8)\n",
    "        x = self.dec_conv2(tf.keras.layers.concatenate([x, x7]))\n",
    "        x = self.dec_conv3(tf.keras.layers.concatenate([x, x6]))\n",
    "        x = self.dec_conv4(tf.keras.layers.concatenate([x, x5]))\n",
    "        x = self.dec_conv5(tf.keras.layers.concatenate([x, x4]))\n",
    "        x = self.dec_conv6(tf.keras.layers.concatenate([x, x3]))\n",
    "        x = self.dec_conv7(tf.keras.layers.concatenate([x, x2]))\n",
    "        x = self.dec_conv8(tf.keras.layers.concatenate([x, x1]))\n",
    "        \n",
    "        # get three channels\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the combined pix2pix model\n",
    "Adter we have described the overall structure and defined the the models for the discriminator and the generator, we define one model that combines both components and implements a training procedure. to eas up the use later on. To do so we derive from the keras base Model which provides basic functions to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pix2pix(Model):\n",
    "    \"\"\"This model implements the Pix2Pix neural network to convert one image into another. \n",
    "    It uses a Generator with UNet encoder and a 70x70 discriminator by default.\n",
    "    \"\"\"\n",
    "    def __init__(self, discriminator=Discriminator70(), output_dim=3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            discriminator: instance of the discriminator to use. (defaults to 70x70 discriminator)\n",
    "            output_dim: dimension of output of generator (defaults to 3 for rgb)\n",
    "        \"\"\"\n",
    "        super(Pix2pix, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.g = Generator(output_dim=output_dim)\n",
    "        self.d = discriminator\n",
    "        \n",
    "        self.cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5, beta_2=0.999)\n",
    "    \n",
    "    def _disc_loss(self, disc_real, disc_fake):\n",
    "        \"\"\"Calculates the loss for the discriminator based on the discriminator output of the real and fake image.\n",
    "        Args:\n",
    "            disc_real: discriminator output for real image\n",
    "            disc_fake: discriminator outpur for generated (fake) image\n",
    "        \"\"\"\n",
    "        return tf.reduce_mean(-tf.math.log(disc_real + 1e-16) - tf.math.log(1 - disc_fake +  + 1e-16))\n",
    "    \n",
    "    def _gen_loss(self, y, generated, disc_fake):\n",
    "        \"\"\"Calculates the loss for the generator based on the output, the generated image \n",
    "        and the discriminator outpur for the generated image.\n",
    "        Args:\n",
    "            y: dataset output\n",
    "            generated: generated output\n",
    "            disc_fake: discriminator output for generated image\n",
    "        \"\"\"\n",
    "        gan_loss = tf.reduce_mean(-tf.math.log(disc_fake + 1e-16))\n",
    "        l1_loss = tf.reduce_mean(tf.abs(y - generated))\n",
    "        return gan_loss + (600 / self.output_dim) * l1_loss\n",
    "    \n",
    "    def split_dataset(self, x, y, validation_split=0.0):\n",
    "        \"\"\"Splits the dataset into test and training dataset. The returned datasets are numpy arrays.\n",
    "        Args:\n",
    "            x: original input images of dataset\n",
    "            y: original input images of dataset\n",
    "            validation_split: split point for splitting the given x`s and y`s into test dataset and training dataset. if set to 0.1, there will be 10% of the given dataset in the testing dataset an 90% in the training dataset.\n",
    "        \"\"\"\n",
    "        num_test = math.floor(x.shape[0] * validation_split)\n",
    "\n",
    "        test_x = np.array(x[:num_test], dtype=np.float32)\n",
    "        test_y = np.array(y[:num_test], dtype=np.float32)\n",
    "\n",
    "        train_x = np.array(x[num_test:], dtype=np.float32)\n",
    "        train_y = np.array(y[num_test:], dtype=np.float32)\n",
    "        \n",
    "        return ((train_x, train_y), (test_x, test_y))\n",
    "    \n",
    "    def predict_on_batch(self, x):\n",
    "        \"\"\"Returns the output of the generator for the given batch x.\n",
    "        Args:\n",
    "            x: input images of batch\n",
    "        \"\"\"\n",
    "        return self.g(x)\n",
    "    \n",
    "    def predict(self, x, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10, workers=1, use_multiprocessing=False):\n",
    "        \"\"\"Generates the output for a input dataset x. The data is expexted to be a numpy array and will be split \n",
    "        up into batches if a batch_size is given.\n",
    "        Args:\n",
    "            x: numpy array of input images\n",
    "            batch_size: size of batches the inputs should be split up into\n",
    "            verbose: [unused]\n",
    "            steps: maximum amount of prediction runs. if the length of x is larger than steps, the output array will have steps entries\n",
    "            calbacks: list of all calbacks that should be called at given events\n",
    "            max_queue_size: [unused]\n",
    "            workers: [unused]\n",
    "            use_multiprocessing: [unused]\n",
    "        \"\"\"\n",
    "        \n",
    "        # generate dataset\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(x)\n",
    "        if batch_size == None:\n",
    "            dataset = dataset.batch(batch_size=1)\n",
    "        else:\n",
    "            dataset = dataset.batch(batch_size=batch_size)\n",
    "        \n",
    "        # call callback for predition start\n",
    "        if callbacks != None:\n",
    "            for callback in callbacks:\n",
    "                callback.on_predict_begin()\n",
    "        \n",
    "        result = np.array([])\n",
    "        \n",
    "        # run for all elements in dataset\n",
    "        for n, x1 in dataset.enumerate():\n",
    "            \n",
    "            # call callback for batch begin\n",
    "            if callbacks != None:\n",
    "                for callback in callbacks:\n",
    "                    callback.on_predict_batch_begin(n)\n",
    "                    \n",
    "            # predict for current batch and add resulting images to results array\n",
    "            output = self.predict_on_batch(x1)\n",
    "            if result.size == 0:\n",
    "                result = output.numpy()\n",
    "            else:\n",
    "                result = np.concatenate((result, output.numpy()), axis=0)\n",
    "            \n",
    "            # call callback for batch end\n",
    "            if callbacks != None:\n",
    "                for callback in callbacks:\n",
    "                    callback.on_predict_batch_end(n)\n",
    "            \n",
    "            if steps != None and n >= steps:\n",
    "                break\n",
    "        \n",
    "        # call callback for prediction end \n",
    "        if callbacks != None:\n",
    "            for callback in callbacks:\n",
    "                callback.on_predict_end()\n",
    "                \n",
    "        return result\n",
    "    \n",
    "    def fit(self, x, y, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False):\n",
    "        \"\"\"Trains the pix2pix model including the discriminator and the generator according to the training strategy described above.\n",
    "        Args:\n",
    "            x: numpy array containing all input images\n",
    "            y: numpy array containing all output images of the dataset\n",
    "            batch_size: size of the batches while training\n",
    "            epochs: number of epochs to train\n",
    "            verbose: [unused]\n",
    "            calbacks: list of all calbacks that should be called at given events\n",
    "            validation_split: split point for splitting the given x`s and y`s into test dataset and training dataset. if set to 0.1, there will be 10% of the given dataset in the testing dataset an 90% in the training dataset.\n",
    "            validation_data: set of (x`s, y`s) used as testing dataset. This can be used instead ov validation_split\n",
    "            shuffle: [unused]\n",
    "            class_weight: [unused]\n",
    "            sample_weight: [unused]\n",
    "            initial_epoch: number of epoch to start\n",
    "            steps_per_epoch: [unused]\n",
    "            validation_steps: [unused]\n",
    "            validation_freq: [unused]\n",
    "            max_queue_size: [unused]\n",
    "            workers: [unused]\n",
    "            use_multiprocessing: [unused]\n",
    "        \"\"\"\n",
    "        \n",
    "        # get or generate validation data\n",
    "        if validation_data != None:\n",
    "            train_x = x\n",
    "            train_y = y\n",
    "            \n",
    "            test_x = validation_data[0]\n",
    "            test_y = validation_data[1]\n",
    "        else:\n",
    "            ((train_x, train_y), (test_x, test_y)) = self.split_dataset(x, y, validation_split=validation_split)\n",
    "        \n",
    "        # generate tf datasets from data\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y))\n",
    "\n",
    "        if shuffle:\n",
    "            train_dataset = train_dataset.shuffle(buffer_size=50000)\n",
    "\n",
    "        if batch_size == None:\n",
    "            train_dataset = train_dataset.batch(batch_size=1)\n",
    "        else:\n",
    "            train_dataset = train_dataset.batch(batch_size=batch_size)\n",
    "        test_dataset = test_dataset.batch(10000)\n",
    "        \n",
    "        # prepare callbacks\n",
    "        if callbacks != None:\n",
    "            params = {\n",
    "                'batch_size': batch_size,\n",
    "                'epochs': epochs,\n",
    "                'steps': steps_per_epoch,\n",
    "                'samples': train_x.shape[0],\n",
    "                'verbose': verbose,\n",
    "                'do_validation': validation_steps != None,\n",
    "                'metrics': []\n",
    "            }\n",
    "            for callback in callbacks:\n",
    "                callback.set_params(params)\n",
    "                \n",
    "                callback.set_model(self)\n",
    "        \n",
    "        # call callbacks for train begin\n",
    "        if callbacks != None:\n",
    "            for callback in callbacks:\n",
    "                callback.on_train_begin()\n",
    "        \n",
    "        # train\n",
    "        for epoch in range(initial_epoch, epochs):\n",
    "            # call callbacks for epoch begin\n",
    "            if callbacks != None:\n",
    "                for callback in callbacks:\n",
    "                    callback.on_epoch_begin(epoch)\n",
    "                    \n",
    "            sum_disc_loss = 0\n",
    "            sum_gen_loss = 0\n",
    "            \n",
    "            for n, (x, y) in train_dataset.enumerate():\n",
    "                # call callbacks for bach begin\n",
    "                if callbacks != None:\n",
    "                    for callback in callbacks:\n",
    "                        callback.on_train_batch_begin(n, logs={\n",
    "                            'size': x.shape[0], \n",
    "                            'batch': n\n",
    "                        })\n",
    "                        \n",
    "                losses = model.train_on_batch(x, y)\n",
    "                sum_disc_loss += losses[0]\n",
    "                sum_gen_loss += losses[1]\n",
    "                \n",
    "                # call callbacks for bach end\n",
    "                if callbacks != None:\n",
    "                    for callback in callbacks:\n",
    "                        callback.on_train_batch_end(n, logs={\n",
    "                            'size': x.shape[0], \n",
    "                            'batch': n,\n",
    "                            'discriminator_loss': losses[0],\n",
    "                            'generator_loss': losses[1],\n",
    "                        })\n",
    "                \n",
    "                if steps_per_epoch != None and n >= steps_per_epoch:\n",
    "                    break\n",
    "                    \n",
    "            # call callbacks for epoch end\n",
    "            if callbacks != None:\n",
    "                epoch_logs = {\n",
    "                    'discriminator_loss': sum_disc_loss / n,\n",
    "                    'generator_loss': sum_gen_loss / n,\n",
    "                }\n",
    "                if test_x.size > 0:\n",
    "                    losses = self.test_on_batch(test_x, test_y)\n",
    "                    epoch_logs['val_discriminator_loss'] = losses[0]\n",
    "                    epoch_logs['val_generator_loss'] = losses[1]\n",
    "                \n",
    "                for callback in callbacks:\n",
    "                    callback.on_epoch_end(epoch, logs=epoch_logs)\n",
    "        \n",
    "        # call callbacks for train end\n",
    "        if callbacks != None:\n",
    "            for callback in callbacks:\n",
    "                callback.on_train_end()\n",
    "    \n",
    "    def train_on_batch(self, x, y, sample_weight=None, class_weight=None, reset_metrics=True):\n",
    "        \"\"\"trains the model on one batch.\n",
    "        Args:\n",
    "            x: x`s of current batch\n",
    "            y: y`s of current batch\n",
    "            sample_weight: [unused]\n",
    "            class_weight: [unused]\n",
    "            reset_metrics: [unused]\n",
    "        \"\"\"\n",
    "        \n",
    "        # train discriminator\n",
    "        with tf.GradientTape() as disc_tape, tf.GradientTape() as gen_tape:\n",
    "            generated = self.g(x)\n",
    "            disc_real = self.d(x, y)\n",
    "            disc_fake = self.d(x, generated)\n",
    "            \n",
    "            disc_loss = self._disc_loss(disc_real, disc_fake)\n",
    "            gen_loss = self._gen_loss(y, generated, disc_fake)\n",
    "            \n",
    "        gradients = disc_tape.gradient(disc_loss, self.d.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.d.trainable_variables))\n",
    "        \n",
    "        gradients = gen_tape.gradient(gen_loss, self.g.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.g.trainable_variables))\n",
    "        \n",
    "        return np.array([disc_loss, gen_loss])\n",
    "        \n",
    "    def test_on_batch(self, x, y, sample_weight=None, reset_metrics=True):\n",
    "        \"\"\"calculates discriminator- and generator loss for given batch.\n",
    "        Args:\n",
    "            x: x´s of batch\n",
    "            y: y´s of batch\n",
    "            sample_weight: [unused]\n",
    "            reset_metrics: [unused]\n",
    "        \"\"\"\n",
    "        generated = self.g(x)\n",
    "        disc_real = self.d(x, y)\n",
    "        disc_fake = self.d(x, generated)\n",
    "\n",
    "        disc_loss = self._disc_loss(disc_real, disc_fake)\n",
    "        gen_loss = self._gen_loss(y, generated, disc_fake)\n",
    "        \n",
    "        return np.array([disc_loss, gen_loss])\n",
    "    \n",
    "    def call(self, x):\n",
    "        \"\"\"Takes input image x and calls generator and discriminator for generator output.\n",
    "        \"\"\"\n",
    "        generated = self.g(x)\n",
    "        discriminator = self.d(x, generated)\n",
    "        \n",
    "        return (generated, discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
